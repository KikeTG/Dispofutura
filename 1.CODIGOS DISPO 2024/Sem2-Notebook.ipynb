{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  COPIAR ARCHIVO BASE SP, MAESTRA AFM, DEFINIR CARPETA DE DESTINO PRINCIPAL, CREAR CARPETA SEMANA ACTUAL, TUBO SEMANAL, STOCK TIENDA, STOCK, TRANSITO CONSOLIDADO\n",
    "# ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ocurrió un error al copiar el archivo: [WinError 32] El proceso no tiene acceso al archivo porque está siendo utilizado por otro proceso\n",
      "El archivo del OTB Aprobado ya existe en la carpeta de destino: \"C:\\Users\\Etorres\\OneDrive - Inchcape\\Archivo Base Dispo\\Sem 11\\SP Marzo 2024 (Cierre) con kumho v2.xlsx\"\n",
      "Archivo COD_ACTUAL_S4_20240201.xlsx copiado exitosamente a C:\\Users\\Etorres\\OneDrive - Inchcape\\Archivo Base Dispo\\Sem 11\n",
      "Archivo \"2024-03-11 Stock Tiendas.XLSX\" copiado a la carpeta destino \"C:\\Users\\Etorres\\OneDrive - Inchcape\\Archivo Base Dispo\\Sem 11\".\n",
      "Archivo \"2024-03-11 - Stock S4.xlsx\" copiado a la carpeta destino \"C:\\Users\\Etorres\\OneDrive - Inchcape\\Archivo Base Dispo\\Sem 11\".\n",
      "El archivo \"2024-03-11 TR FINAL S4 Consolidado.xlsx\" ya existe en la carpeta destino \"C:\\Users\\Etorres\\OneDrive - Inchcape\\Archivo Base Dispo\\Sem 11\".\n",
      "El archivo 02.2024 S&OP Demanda Sin restricciones Inbound Ciclo Mar 24_Inbound.xlsx ha sido copiado a C:\\Users\\Etorres\\OneDrive - Inchcape\\Archivo Base Dispo\\Sem 11 exitosamente.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "user_home_dir = os.path.expanduser('~')\n",
    "\n",
    "# Obtener el mes actual en formato de nombre (por ejemplo, 'octubre')\n",
    "mes_actual_numero = datetime.now().strftime('%m')\n",
    "mes_actual = datetime.now().strftime('%B').lower()\n",
    "\n",
    "# Mapeo de nombres de meses en inglés a español\n",
    "meses = {\n",
    "    'january': 'enero',\n",
    "    'february': 'febrero',\n",
    "    'march': 'marzo',\n",
    "    'april': 'abril',\n",
    "    'may': 'mayo',\n",
    "    'june': 'junio',\n",
    "    'july': 'julio',\n",
    "    'august': 'agosto',\n",
    "    'september': 'septiembre',\n",
    "    'october': 'octubre',\n",
    "    'november': 'noviembre',\n",
    "    'december': 'diciembre'\n",
    "}\n",
    "\n",
    "# Obtener el nombre del mes en español\n",
    "mes_actual = meses.get(mes_actual, mes_actual)\n",
    "\n",
    "# Ruta de la carpeta donde se encuentran los archivos\n",
    "# carpeta_origen = r'C:\\Users\\Etorres\\OneDrive - Inchcape\\Open to Buy_OTB\\2023 OK'\n",
    "\n",
    "carpeta_origen = os.path.join(user_home_dir, 'Inchcape', 'Planificación y Compras Chile - Documentos', 'Planificación y Compras Aftermarket', 'Open to Buy_OTB', '2024')\n",
    "\n",
    "# RUTA DEL \"C:\\Users\\Etorres\\OneDrive - Inchcape\\Maestros Actualizables\\Maestra Aftermarket Actualizable.xlsx\"\n",
    "# ruta_AFMACTUALIZABLE = r'C:\\Users\\Etorres\\OneDrive - Inchcape\\Maestros Actualizables\\Maestra Aftermarket Actualizable.xlsx'\n",
    "\n",
    "# ruta_AFMACTUALIZABLE = os.path.join(user_home_dir, 'OneDrive - Inchcape', 'Maestros Actualizables', 'Maestra Aftermarket Actualizable.xlsx')\n",
    "\n",
    "ruta_AFMACTUALIZABLE = os.path.join(user_home_dir, 'Inchcape', 'Planificación y Compras Chile - Documentos', 'Planificación y Compras Maestros', 'Vigencias', 'Maestra Retail', 'Maestra Aftermarket Actualizable.xlsx')\n",
    "\n",
    "\n",
    "# Identificar la subcarpeta que contiene el mes actual en su nombre\n",
    "subcarpetas = [d for d in os.listdir(carpeta_origen) if os.path.isdir(os.path.join(carpeta_origen, d))]\n",
    "subcarpeta_mes = next((d for d in subcarpetas if mes_actual in d.lower()), None)\n",
    "\n",
    "if subcarpeta_mes:\n",
    "    carpeta_origen_mes = os.path.join(carpeta_origen, subcarpeta_mes)\n",
    "else:\n",
    "    raise ValueError(f\"No se encontró ninguna subcarpeta que contenga el mes '{mes_actual}' en su nombre.\")\n",
    "\n",
    "archivos_otb = [f for f in os.listdir(carpeta_origen_mes) \n",
    "                if os.path.isfile(os.path.join(carpeta_origen_mes, f)) \n",
    "                and 'SP' in f \n",
    "                and mes_actual in f.lower()]\n",
    "\n",
    "if not archivos_otb:\n",
    "    raise ValueError(f\"No se encontraron archivos del OTB Aprobado que contengan 'SP' y '{mes_actual}' en su nombre.\")\n",
    "\n",
    "# Seleccionar el archivo más reciente\n",
    "archivo_otb_reciente = max(archivos_otb, key=lambda x: os.path.getmtime(os.path.join(carpeta_origen_mes, x)))\n",
    "\n",
    "# Ruta completa del archivo OTB Aprobado más reciente\n",
    "ruta_otb_reciente = os.path.join(carpeta_origen_mes, archivo_otb_reciente) \n",
    "\n",
    "\n",
    "# carpeta_destino_principal = r'C:\\Users\\Etorres\\OneDrive - Inchcape\\Archivo Base Dispo'\n",
    "\n",
    "carpeta_destino_principal = os.path.join(user_home_dir, 'OneDrive - Inchcape', 'Archivo Base Dispo')\n",
    "\n",
    "\n",
    "# Obtener el número de la semana actual\n",
    "semana_actual = datetime.now().isocalendar()[1]\n",
    "\n",
    "# Ruta de la carpeta de destino con la semana actual\n",
    "carpeta_destino_semana = os.path.join(carpeta_destino_principal, f'Sem {semana_actual}')\n",
    "\n",
    "# Verificar si la carpeta de destino con la semana actual existe o no\n",
    "if not os.path.exists(carpeta_destino_semana):\n",
    "    # Si no existe, crear la carpeta\n",
    "    os.makedirs(carpeta_destino_semana)\n",
    "\n",
    "\n",
    "# Verificar si el archivo \"Maestra Aftermarket Actualizable.xlsx\" existe\n",
    "if os.path.exists(ruta_AFMACTUALIZABLE):\n",
    "    try:\n",
    "        # Copiar el archivo \"Maestra Aftermarket Actualizable.xlsx\" a la carpeta de la semana actual\n",
    "        shutil.copy2(ruta_AFMACTUALIZABLE, carpeta_destino_semana)\n",
    "        print(f'Archivo \"Maestra Aftermarket Actualizable.xlsx\" copiado a la carpeta de la semana actual.')\n",
    "        \n",
    "        # Verificar si la copia fue exitosa\n",
    "        archivo_copiado = os.path.join(carpeta_destino_semana, os.path.basename(ruta_AFMACTUALIZABLE))\n",
    "        if os.path.exists(archivo_copiado):\n",
    "            print(\"La copia del archivo se realizó correctamente.\")\n",
    "        else:\n",
    "            print(\"No se encontró el archivo copiado en la carpeta de destino.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error al copiar el archivo: {e}\")\n",
    "else:\n",
    "    print(f'El archivo \"{ruta_AFMACTUALIZABLE}\" no existe o no se encuentra en la ruta especificada.')\n",
    "\n",
    "\n",
    "ruta_destino_otb = os.path.join(carpeta_destino_semana, archivo_otb_reciente)\n",
    "if os.path.exists(ruta_destino_otb):\n",
    "    print(f'El archivo del OTB Aprobado ya existe en la carpeta de destino: \"{ruta_destino_otb}\"')\n",
    "else:\n",
    "    # Copiar el archivo del OTB Aprobado a la carpeta de destino\n",
    "    shutil.copy2(ruta_otb_reciente, carpeta_destino_semana)\n",
    "    print(f'Archivo del OTB Aprobado copiado a la carpeta de destino.')\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Obtener el primer día del mes actual\n",
    "primer_dia_mes_actual = datetime.now().replace(day=1)\n",
    "\n",
    "# Restar un día para obtener el último día del mes anterior\n",
    "ultimo_dia_mes_anterior = primer_dia_mes_actual - timedelta(days=1)\n",
    "\n",
    "# Extraer el mes anterior\n",
    "mes_anterior_numero = ultimo_dia_mes_anterior.strftime('%m')\n",
    "\n",
    "# cod actual s4 mes anterior\n",
    "# ruta_base = \"C:\\\\Users\\\\Etorres\\\\OneDrive - Inchcape\\\\Planificación y Compras Maestros\\\\2023\\\\\"\n",
    "ruta_base = os.path.join(user_home_dir, \"Inchcape\", \"Planificación y Compras Chile - Documentos\", \"Planificación y Compras Maestros\", \"2024\")\n",
    "\n",
    "nombre_carpeta = f\"2024-{mes_anterior_numero}\"\n",
    "ruta_carpeta_mes = os.path.join(ruta_base, nombre_carpeta)\n",
    "\n",
    "archivo_encontrado = None\n",
    "for archivo in os.listdir(ruta_carpeta_mes):\n",
    "    if archivo.startswith(\"COD_ACTUAL_S4\"):\n",
    "        archivo_encontrado = archivo\n",
    "        break\n",
    "\n",
    "if archivo_encontrado:\n",
    "    ruta_origen = os.path.join(ruta_carpeta_mes, archivo_encontrado)\n",
    "    shutil.copy(ruta_origen, carpeta_destino_semana)\n",
    "    print(f\"Archivo {archivo_encontrado} copiado exitosamente a {carpeta_destino_semana}\")\n",
    "else:\n",
    "    print(\"No se encontró el archivo que comienza con 'COD_ACTUAL_S4'\")\n",
    "def es_formato_fecha_valido(nombre_carpeta):\n",
    "    try:\n",
    "        datetime.strptime(nombre_carpeta, \"%Y-%m-%d\")\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Ruta donde están las carpetas con fechas\n",
    "# ruta_kpi_reportes = r'C:\\Users\\Etorres\\OneDrive - Inchcape\\Planificación y Compras KPI-Reportes\\Tubo Semanal'\n",
    "\n",
    "ruta_kpi_reportes = os.path.join(user_home_dir, 'Inchcape', 'Planificación y Compras Chile - Documentos','Planificación y Compras KPI-Reportes', 'Tubo Semanal')\n",
    "\n",
    "# Obtener una lista de todas las carpetas en la ruta_kpi_reportes que cumplan con el formato de fecha\n",
    "carpetas = [d for d in os.listdir(ruta_kpi_reportes) if os.path.isdir(os.path.join(ruta_kpi_reportes, d)) and es_formato_fecha_valido(d)]\n",
    "\n",
    "# Ordenar las carpetas por su fecha\n",
    "carpetas_ordenadas = sorted(carpetas, key=lambda x: datetime.strptime(x, \"%Y-%m-%d\"), reverse=True)\n",
    "\n",
    "# Tomar la carpeta más cercana a la fecha actual (la primera en la lista ordenada)\n",
    "carpeta_mas_cercana = carpetas_ordenadas[0]\n",
    "\n",
    "# Ruta completa de la carpeta seleccionada\n",
    "ruta_carpeta_seleccionada = os.path.join(ruta_kpi_reportes, carpeta_mas_cercana)\n",
    "\n",
    "# Buscar el archivo que contiene \"Stock Tiendas\" en su nombre\n",
    "archivo_stock_tiendas = next((f for f in os.listdir(ruta_carpeta_seleccionada) if \"Stock Tiendas\" in f), None)\n",
    "\n",
    "# Si encontramos el archivo, copiamos a la carpeta_destino_semana\n",
    "if archivo_stock_tiendas:\n",
    "    ruta_origen_stock = os.path.join(ruta_carpeta_seleccionada, archivo_stock_tiendas)\n",
    "    shutil.copy2(ruta_origen_stock, carpeta_destino_semana)\n",
    "    print(f'Archivo \"{archivo_stock_tiendas}\" copiado a la carpeta destino \"{carpeta_destino_semana}\".')\n",
    "else:\n",
    "    print(f\"No se encontró ningún archivo con 'Stock Tiendas' en su nombre en la carpeta {carpeta_mas_cercana}.\")\n",
    "\n",
    "archivo_stock_tiendas2 = next((f for f in os.listdir(ruta_carpeta_seleccionada) if \"Stock S4\" in f and not ('Tiendas' in f or 'Pa' in f or 'Centro' in f or 'R3' in f)), None)\n",
    "\n",
    "#     # Si encontramos el archivo, copiamos a la carpeta_destino_semana\n",
    "if archivo_stock_tiendas2:\n",
    "    ruta_origen_stock2 = os.path.join(ruta_carpeta_seleccionada, archivo_stock_tiendas2)\n",
    "    shutil.copy2(ruta_origen_stock2, carpeta_destino_semana)\n",
    "    print(f'Archivo \"{archivo_stock_tiendas2}\" copiado a la carpeta destino \"{carpeta_destino_semana}\".')\n",
    "else:\n",
    "    print(f\"No se encontró ningún archivo con 'Stock S4' en su nombre en la carpeta {carpeta_mas_cercana}.\")\n",
    "archivo_stock_tiendas3 = next((f for f in os.listdir(ruta_carpeta_seleccionada) if \"onsolidado\" in f and 'S4' in f), None)\n",
    "\n",
    "if archivo_stock_tiendas3:\n",
    "    ruta_origen_stock3 = os.path.join(ruta_carpeta_seleccionada, archivo_stock_tiendas3)\n",
    "    ruta_destino_stock3 = os.path.join(carpeta_destino_semana, archivo_stock_tiendas3)\n",
    "\n",
    "    # Verificar si el archivo ya existe en la carpeta de destino\n",
    "    if not os.path.exists(ruta_destino_stock3):\n",
    "        shutil.copy2(ruta_origen_stock3, carpeta_destino_semana)\n",
    "        print(f'Archivo \"{archivo_stock_tiendas3}\" (CONSOLIDADO) copiado a la carpeta destino \"{carpeta_destino_semana}\".')\n",
    "    else:\n",
    "        print(f'El archivo \"{archivo_stock_tiendas3}\" ya existe en la carpeta destino \"{carpeta_destino_semana}\".')\n",
    "else:\n",
    "    print(f\"No se encontró ningún archivo con 'Stock CONSOLIDADO' en su nombre en la carpeta {ruta_carpeta_seleccionada}.\")\n",
    "\n",
    "# ruta_origen_stock3 = r'C:\\Users\\Etorres\\OneDrive - Inchcape\\Archivo Base Dispo\\Sem 47\\2023-11-13 TR semana 46 Consolidado.xlsx'\n",
    "# ruta_origen_stock2 = r'C:\\Users\\Etorres\\OneDrive - Inchcape\\Archivo Base Dispo\\Sem 47\\2023-11-13 - Stock.XLSX'\n",
    "# # ruta_origen_stock = r'C:\\Users\\Etorres\\OneDrive - Inchcape\\Archivo Base Dispo\\Sem 47\\2023-11-13 Stock Tiendas.XLSX'\n",
    "\n",
    "# ruta_origen_stock3 = os.path.join(carpeta_destino_principal, 'Sem 47', '2023-11-13 TR semana 46 Consolidado.xlsx')\n",
    "# ruta_origen_stock2 = os.path.join(carpeta_destino_principal, 'Sem 47', '2023-11-13 - Stock.XLSX')\n",
    "# ruta_origen_stock = os.path.join(carpeta_destino_principal, 'Sem 47', '2023-11-13 Stock Tiendas.XLSX')\n",
    "\n",
    "    \n",
    "#BUSCAR FORECAST ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "import locale\n",
    "\n",
    "# Ruta donde se encuentran las carpetas por año y mes\n",
    "# directorio_forecast = r\"C:\\Users\\Etorres\\OneDrive - Inchcape\\Forecast Inbound\"\n",
    "\n",
    "directorio_forecast = os.path.join(user_home_dir, 'Inchcape', 'Planificación y Compras Chile - Documentos', 'Planificación y Compras Aftermarket', 'Forecast Inbound')\n",
    "\n",
    "\n",
    "# Mapeo de nombres de meses en inglés a español\n",
    "meses_en_espanol = {\n",
    "    \"January\": \"Enero\",\n",
    "    \"February\": \"Febrero\",\n",
    "    \"March\": \"Marzo\",\n",
    "    \"April\": \"Abril\",\n",
    "    \"May\": \"Mayo\",\n",
    "    \"June\": \"Junio\",\n",
    "    \"July\": \"Julio\",\n",
    "    \"August\": \"Agosto\",\n",
    "    \"September\": \"Septiembre\",\n",
    "    \"October\": \"Octubre\",\n",
    "    \"November\": \"Noviembre\",\n",
    "    \"December\": \"Diciembre\"\n",
    "}\n",
    "\n",
    "# Establece el entorno local a inglés para obtener el mes en inglés\n",
    "locale.setlocale(locale.LC_TIME, 'en_US.UTF-8')\n",
    "\n",
    "# Función para obtener el nombre de la carpeta objetivo\n",
    "def obtener_nombre_carpeta(fecha):\n",
    "    año = fecha.strftime(\"%Y\")\n",
    "    mes_inglés = fecha.strftime(\"%B\")\n",
    "    mes_español = meses_en_espanol[mes_inglés]\n",
    "    mes_número = fecha.strftime(\"%m\")\n",
    "    return f\"{año}-{mes_número} {mes_español}\"\n",
    "\n",
    "# Calcular la fecha del mes anterior\n",
    "fecha_actual = datetime.now()\n",
    "fecha_mes_anterior = fecha_actual - timedelta(days=fecha_actual.day)\n",
    "nombre_carpeta_mes_anterior = obtener_nombre_carpeta(fecha_mes_anterior)\n",
    "\n",
    "# Buscar la carpeta del mes anterior\n",
    "ruta_carpeta_encontrada = \"\"\n",
    "for nombre_carpeta in os.listdir(directorio_forecast):\n",
    "    if nombre_carpeta_mes_anterior in nombre_carpeta:\n",
    "        ruta_carpeta_encontrada = os.path.join(directorio_forecast, nombre_carpeta)\n",
    "        break\n",
    "\n",
    "# Si se encontró la carpeta del mes anterior\n",
    "if ruta_carpeta_encontrada:\n",
    "    # Buscar el archivo con la palabra \"Inbound\" más reciente\n",
    "    lista_archivos = [archivo for archivo in os.listdir(ruta_carpeta_encontrada) if \"Inbound\" in archivo]\n",
    "    lista_archivos.sort(key=lambda archivo: os.path.getmtime(os.path.join(ruta_carpeta_encontrada, archivo)), reverse=True)\n",
    "    \n",
    "    if lista_archivos:\n",
    "        archivo_reciente = lista_archivos[0]\n",
    "        ruta_completa_archivo = os.path.join(ruta_carpeta_encontrada, archivo_reciente)\n",
    "        \n",
    "        # Copiar el archivo más reciente a la carpeta destino\n",
    "        shutil.copy(ruta_completa_archivo, carpeta_destino_semana)\n",
    "        print(f\"El archivo {archivo_reciente} ha sido copiado a {carpeta_destino_semana} exitosamente.\")\n",
    "    else:\n",
    "        print(f\"No se encontraron archivos con la palabra 'Inbound' en {ruta_carpeta_encontrada}.\")\n",
    "else:\n",
    "    print(f\"No se encontró la carpeta para el mes anterior.\")\n",
    "\n",
    "\n",
    "#COPIAR Y PEGAR LO DEL SP Y CALCULAR LEADTIME SEMANAL  ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "# Asumiendo que 'ruta_destino_otb' ya está definida con la ruta al archivo de Excel.\n",
    "try:\n",
    "    # Intentar cargar los datos desde la hoja \"Base\"\n",
    "    df = pd.read_excel(ruta_destino_otb, sheet_name=\"Base\", usecols=\"A:B, D:I, K:L, N, CX, CY, ER, EU, M, ET, DM:DN\", skiprows=1)\n",
    "except FileNotFoundError:\n",
    "    print(\"El archivo especificado no fue encontrado.\")\n",
    "except ValueError as e:\n",
    "    if \"Worksheet named 'Base' not found\" in str(e):\n",
    "        try:\n",
    "            # Si la hoja \"Base\" no existe, intentar cargar desde la hoja \"Base A\"\n",
    "            df = pd.read_excel(ruta_destino_otb, sheet_name=\"Base A\", usecols=\"A:B, D:I, K:L, N, CX, CY, ER, EU, M, ET, DM:DN\", skiprows=1)\n",
    "        except FileNotFoundError:\n",
    "            print(\"El archivo especificado no fue encontrado.\")\n",
    "        except ValueError:\n",
    "            print(\"Ni la hoja 'Base' ni la hoja 'Base A' se encontraron en el archivo.\")\n",
    "    else:\n",
    "        print(\"Ocurrió un error al leer el archivo:\", e)\n",
    "\n",
    "df.insert(11, 'Leadtime Semanal', (df.iloc[:, 10] / 7).round(2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCULO VIGENCIA ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "nombres_columnas = [\"Material SAP\", \"Condicion de Compra\", \"CES 01\", \"Mayorista\", \"Sodimac\", \"Easy\", \"Walmart\", \"SMU\", \"Tottus\", \"Retail (AP-AG)\"]\n",
    "ruta_archivo_2 = os.path.join(carpeta_destino_semana, \"Maestra Aftermarket Actualizable.xlsx\")\n",
    "dataframe_maestra_2 = pd.read_excel(ruta_archivo_2, usecols=nombres_columnas)#, skiprows=1)\n",
    "\n",
    "agrupado_2 = dataframe_maestra_2.groupby(by=[\"Material SAP\", \"Condicion de Compra\"]).sum().reset_index()\n",
    "\n",
    "dataframe_maestra_2['vig may'] = ((dataframe_maestra_2['CES 01'] + dataframe_maestra_2['Mayorista']) != 0).astype(int)\n",
    "\n",
    "dataframe_maestra_2['vig gt'] = ((dataframe_maestra_2[['Sodimac', 'Easy', 'Walmart', 'SMU', 'Tottus']].sum(axis=1)) != 0).astype(int)\n",
    "\n",
    "cond1 = dataframe_maestra_2['Retail (AP-AG)'] != 0\n",
    "cond2 = dataframe_maestra_2['Condicion de Compra'] != 2\n",
    "dataframe_maestra_2['vig retail'] = (cond1 & cond2).astype(int)\n",
    "\n",
    "dataframe_maestra_2['vig total'] = ((dataframe_maestra_2[['vig may', 'vig gt', 'vig retail']].sum(axis=1)) != 0).astype(int)\n",
    "\n",
    "#LEER CODIGO S4 MES ANTERIOR ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COD_ACTUAL_S4_20230301.xlsx'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archivo_cod_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "archivo_cod_actual = None\n",
    "for archivo in os.listdir(carpeta_destino_semana):\n",
    "    if archivo.startswith(\"COD_ACTUAL_S4\"):\n",
    "        archivo_cod_actual = archivo\n",
    "        break\n",
    "\n",
    "# 2. Leer las columnas requeridas del archivo encontrado\n",
    "ruta_archivo_cod_actual = os.path.join(carpeta_destino_semana, archivo_cod_actual)\n",
    "df_cod_actual = pd.read_excel(ruta_archivo_cod_actual, usecols=[\"Nro_pieza_fabricante_1\", \"Cod_Actual_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Etorres\\\\OneDrive - Inchcape\\\\Archivo Base Dispo\\\\Sem 11\\\\COD_ACTUAL_S4_20240201.xlsx'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruta_archivo_cod_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo Falt & Sobr AP Y AGP_01.03.24 PV.xlsx copiado exitosamente a C:\\Users\\Etorres\\OneDrive - Inchcape\\Archivo Base Dispo\\Sem 11\n",
      "       Ultimo Slabon  Faltante  Sobrante\n",
      "0             100483        20       0.0\n",
      "1             100484         7       0.0\n",
      "2             100485        15       0.0\n",
      "3             100486        41     129.0\n",
      "4             100487        49       6.0\n",
      "...              ...       ...       ...\n",
      "10771        1167587        18       0.0\n",
      "10772        1167588        24       0.0\n",
      "10773        1167589        32       0.0\n",
      "10774        1167590        37       0.0\n",
      "10775        1167595        11       0.0\n",
      "\n",
      "[10776 rows x 3 columns]\n",
      "¿Hay duplicados?: False\n",
      "Número de duplicados: 0\n",
      "Empty DataFrame\n",
      "Columns: [Material, Libre utilización]\n",
      "Index: []\n",
      "Valores únicos en Alm.: ['1100' '0' '1600']\n",
      "Valores únicos en Ce.: ['714' '711' '710' '496' '712']\n",
      "¿Hay duplicados?: False\n",
      "Número de duplicados: 0\n",
      "Empty DataFrame\n",
      "Columns: [Material, Libre utilización, Insp. Calidad, Traslado, Control]\n",
      "Index: []\n",
      "¿Hay duplicados?: True\n",
      "Número de duplicados: 2890\n",
      "      Centro Almacén  Libre utilización  Traslado  Insp. Calidad  \\\n",
      "24       714       0                0.0         1              0   \n",
      "25       714    1100                4.0         0              0   \n",
      "26       714       0                0.0         1              0   \n",
      "27       714    1600                0.0         0              0   \n",
      "35       714    1100             1150.0         0              0   \n",
      "...      ...     ...                ...       ...            ...   \n",
      "15416    714    1100                2.0         0              0   \n",
      "15424    714    1100                7.0         0              0   \n",
      "15425    714    1600                0.0         0              0   \n",
      "15499    714    1100               77.0         0              0   \n",
      "15500    714    1600                0.0         0              0   \n",
      "\n",
      "      Nro_pieza_fabricante_1 Material  \n",
      "24                       NaN   101104  \n",
      "25                       NaN   101104  \n",
      "26                       NaN   101132  \n",
      "27                       NaN   101132  \n",
      "35                       NaN   101796  \n",
      "...                      ...      ...  \n",
      "15416                1161754  1161754  \n",
      "15424                1161777  1161777  \n",
      "15425                1161777  1161777  \n",
      "15499                    NaN  1167586  \n",
      "15500                    NaN  1167586  \n",
      "\n",
      "[5387 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convertir las columnas a strings para asegurarse de que tengan el mismo tipo de datos\n",
    "dataframe_maestra_2['Material SAP'] = dataframe_maestra_2['Material SAP'].astype(str)\n",
    "df_cod_actual['Nro_pieza_fabricante_1'] = df_cod_actual['Nro_pieza_fabricante_1'].astype(str)\n",
    "\n",
    "# 3. Realizar merge\n",
    "dataframe_maestra_2 = dataframe_maestra_2.merge(df_cod_actual, left_on='Material SAP', right_on='Nro_pieza_fabricante_1', how='left')\n",
    "\n",
    "# 4. Si no hay coincidencia en el cruce, llenar \"Cod_Actual_1\" con el valor de \"Material SAP\"\n",
    "dataframe_maestra_2[\"Cod_Actual_1\"].fillna(dataframe_maestra_2[\"Material SAP\"], inplace=True)\n",
    "\n",
    "# 5. Renombrar la columna y eliminar la columna adicional\n",
    "dataframe_maestra_2.rename(columns={\"Cod_Actual_1\": \"COD ACTUAL\"}, inplace=True)\n",
    "dataframe_maestra_2.drop(columns='Nro_pieza_fabricante_1', inplace=True)\n",
    "\n",
    "# groupby\n",
    "df_agrupado = dataframe_maestra_2.groupby('COD ACTUAL').agg({\n",
    "    'vig may': 'sum',\n",
    "    'vig gt': 'sum',\n",
    "    'vig retail': 'sum',\n",
    "    'vig total': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Reemplazar valores mayores a 1 por 1\n",
    "cols = ['vig may', 'vig gt', 'vig retail', 'vig total']\n",
    "for col in cols:\n",
    "    df_agrupado[col] = df_agrupado[col].apply(lambda x: 1 if x > 1 else x)\n",
    "\n",
    "df_agrupado.head()  \n",
    "\n",
    "# Convertir las columnas 'Material' y 'COD ACTUAL' a tipo object\n",
    "df['Material'] = df['Material'].astype(str)\n",
    "df_agrupado['COD ACTUAL'] = df_agrupado['COD ACTUAL'].astype(str)\n",
    "\n",
    "# Hacer el merge, solo con las columnas especificadas\n",
    "df_resultado = df.merge(df_agrupado[['COD ACTUAL', 'vig may', 'vig gt', 'vig retail']],\n",
    "                        left_on='Material', \n",
    "                        right_on='COD ACTUAL', \n",
    "                        how='left')\n",
    "\n",
    "# Rellenar con 0 en caso de que no haya coincidencias\n",
    "for columna in ['vig may', 'vig gt', 'vig retail']:\n",
    "    df_resultado[columna].fillna(0, inplace=True)\n",
    "\n",
    "# Eliminar la columna 'COD ACTUAL'\n",
    "df_resultado = df_resultado.drop('COD ACTUAL', axis=1)\n",
    "\n",
    "# Crear la columna 'Vigencia Total'\n",
    "df_resultado['Vigencia Total'] = df_resultado[['vig may', 'vig gt', 'vig retail']].sum(axis=1).apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "# Reemplazar df con el dataframe resultante\n",
    "df = df_resultado\n",
    "\n",
    "# OBSOLECENCIA ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Obtener la lista de columnas del dataframe\n",
    "columnas = df_resultado.columns.tolist()\n",
    "\n",
    "# Remover 'OBS Retail' y 'OBS DERCO' de la lista\n",
    "columnas.remove('OBS Retail')\n",
    "columnas.remove('OBS DERCO')\n",
    "\n",
    "# Ajustar los valores negativos a 0 en 'OBS Retail' y 'OBS DERCO'\n",
    "df_resultado['OBS Retail'] = df_resultado['OBS Retail'].apply(lambda x: 0 if x < 0 else x)\n",
    "df_resultado['OBS DERCO'] = df_resultado['OBS DERCO'].apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "# Agregar 'OBS Retail' y 'OBS DERCO' al final de la lista\n",
    "columnas.extend(['OBS Retail', 'OBS DERCO'])\n",
    "\n",
    "# Reorganizar las columnas \n",
    "df_resultado = df_resultado[columnas]\n",
    "\n",
    "df_resultado = df_resultado.copy()\n",
    "\n",
    "# Crear la columna 'Obs Total' utilizando .loc\n",
    "df_resultado.loc[:, 'Obs Total'] = (df_resultado['OBS Retail'] + df_resultado['OBS DERCO'] > 0).astype(int)\n",
    "\n",
    "# Agregar 'Obs Total' al final de la lista de columnas\n",
    "columnas.append('Obs Total')\n",
    "\n",
    "# Reorganizar las columnas del dataframe para asegurarse de que 'Obs Total' esté al final\n",
    "df_resultado = df_resultado[columnas]\n",
    " \n",
    "#FALTANTES Y SOBRANTES ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Ruta de origen\n",
    "# ruta_origen = r\"C:\\Users\\Etorres\\OneDrive - Inchcape\\Planificación y Compras KPI-Reportes\\Faltantes y Sobrantes Retail\\Consolidado Sobrantes y Faltantes AP - AGP\"\n",
    "\n",
    "ruta_origen = os.path.join(user_home_dir, 'Inchcape', 'Planificación y Compras Chile - Documentos', 'Planificación y Compras KPI-Reportes', 'Faltantes y Sobrantes Retail', 'Sobrantes y Faltantes AP - AGP')\n",
    "\n",
    "# Lista todos los archivos en la ruta de origen\n",
    "archivos = [f for f in os.listdir(ruta_origen) if os.path.isfile(os.path.join(ruta_origen, f))]\n",
    "\n",
    "# Función para extraer la fecha del nombre del archivo\n",
    "def extraer_fecha(archivo):\n",
    "    # Intenta el primer patrón\n",
    "    match = re.search(r\"(\\d{2}.\\d{2}.\\d{4})\", archivo)\n",
    "    if match:\n",
    "        fecha_str = match.group(1)\n",
    "        return datetime.strptime(fecha_str, \"%d.%m.%Y\")\n",
    "    # Si el primer patrón no funciona, intenta el segundo\n",
    "    match = re.search(r\"(\\d{2}.\\d{2}.\\d{2})\", archivo)\n",
    "    if match:\n",
    "        fecha_str = match.group(1)\n",
    "        return datetime.strptime(fecha_str, \"%d.%m.%y\")\n",
    "    return None\n",
    "\n",
    "# Obtener el mes y año actual\n",
    "mes_actual = datetime.now().month\n",
    "año_actual = datetime.now().year\n",
    "\n",
    "# Si estamos en enero, el mes anterior sería diciembre del año pasado\n",
    "if mes_actual == 1:\n",
    "    mes_anterior = 12\n",
    "    año_anterior = año_actual - 1\n",
    "else:\n",
    "    mes_anterior = mes_actual - 1\n",
    "    año_anterior = año_actual\n",
    "\n",
    "# Filtra solo los archivos que contienen una fecha en su nombre son excel y del mes pasado\n",
    "# archivos_filtrados = [\n",
    "#     f for f in archivos if extraer_fecha(f) \n",
    "#     and extraer_fecha(f).month == mes_anterior \n",
    "#     and extraer_fecha(f).year == año_anterior \n",
    "#     and (f.endswith('.xlsx') or f.endswith('.xls')) \n",
    "#     and \"xD\".lower() not in f.lower()\n",
    "# ]\n",
    "# Filtra solo los archivos que contienen una fecha en su nombre son excel y mes actual\n",
    "archivos_filtrados = [\n",
    "    f for f in archivos if extraer_fecha(f) \n",
    "    and extraer_fecha(f).month == mes_actual   # Cambiado a mes_actual\n",
    "    and extraer_fecha(f).year == año_actual    # Cambiado a año_actual\n",
    "    and (f.endswith('.xlsx') or f.endswith('.xls')) \n",
    "    and \"xD\".lower() not in f.lower()\n",
    "]\n",
    "\n",
    "# Si no hay archivos que cumplan el criterio\n",
    "if not archivos_filtrados:\n",
    "    print(\"No se encontró un archivo que cumpla con los criterios.\")\n",
    "    exit()\n",
    "\n",
    "# Ordena los archivos por fecha\n",
    "archivos_filtrados.sort(key=extraer_fecha, reverse=True)\n",
    "\n",
    "# Toma el archivo con la fecha más cercana al mes actual (o el único archivo si solo hay uno)\n",
    "archivo_a_mover = archivos_filtrados[0]\n",
    "\n",
    "# Copia el archivo a la carpeta de destino\n",
    "shutil.copy(os.path.join(ruta_origen, archivo_a_mover), carpeta_destino_semana)\n",
    "\n",
    "print(f\"Archivo {archivo_a_mover} copiado exitosamente a {carpeta_destino_semana}\")\n",
    "# Define la ruta completa de destino\n",
    "ruta_destino_completa = os.path.join(carpeta_destino_semana, archivo_a_mover)\n",
    "# ruta_destino_completa = r'C:\\Users\\Etorres\\OneDrive - Inchcape\\Archivo Base Dispo\\Sem 47\\Falt & Sobr AP Y AGP_06.11.23 (Mat-Cen).XLSX'\n",
    "# ruta_destino_completa = os.path.join(user_home_dir, 'OneDrive - Inchcape', 'Archivo Base Dispo', 'Sem 47', 'Falt & Sobr AP Y AGP_06.11.23 (Mat-Cen).XLSX')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Función auxiliar para buscar una hoja que contenga 'TB' en su nombre\n",
    "def encontrar_hoja_tb(ruta_destino_completa):\n",
    "    xl = pd.ExcelFile(ruta_destino_completa)\n",
    "    for sheet_name in xl.sheet_names:\n",
    "        if 'TB' in sheet_name:\n",
    "            return sheet_name\n",
    "    return None  # Retornar None si no se encuentra ninguna hoja\n",
    "\n",
    "# Cargar la hoja correcta\n",
    "hoja_tb = encontrar_hoja_tb(ruta_destino_completa)\n",
    "if hoja_tb is None:\n",
    "    print(\"No se encontró una hoja que contenga 'TB' en su nombre.\")\n",
    "    exit()\n",
    "\n",
    "# Cargar solo las primeras 5 columnas\n",
    "df = pd.read_excel(ruta_destino_completa, sheet_name=hoja_tb, usecols=range(5))\n",
    "\n",
    "# Preprocesamiento para manejar variaciones en los nombres de las columnas\n",
    "# Crear un mapeo de nombres esperados a posibles variaciones\n",
    "column_mapping = {\n",
    "    'POG': ['POG'],\n",
    "    'Ultimo Slabon': ['Ultimo Slabon', 'Ultmo Slabon', 'Slabon', 'Último Eslabón'],\n",
    "    'Faltante': ['Faltante', 'Faltantes', 'Faltante '],\n",
    "    'Sobrante': ['Sobrante', 'Sobrantes', 'Sobrante '],\n",
    "    'Stock Objetivo': ['Stock Objetivo', 'Stock Objetivo ']\n",
    "}\n",
    "\n",
    "# Normalizar los nombres de las columnas\n",
    "for expected, variations in column_mapping.items():\n",
    "    for variation in variations:\n",
    "        if variation in df.columns:\n",
    "            df.rename(columns={variation: expected}, inplace=True)\n",
    "\n",
    "# Filtrar por POG = 'AP' y luego realizar el groupby por 'Ultimo Slabon'\n",
    "# Sumar las columnas 'Faltante' y 'Sobrante'\n",
    "Resumen_AP = df[df['POG'] == 'AP'].groupby('Ultimo Slabon')[['Faltante', 'Sobrante']].sum().reset_index()\n",
    "\n",
    "# Mostrar el DataFrame 'Resumen AP'\n",
    "print(Resumen_AP)\n",
    "Resumen_AGP = df[df['POG'] == 'AGP'].groupby('Ultimo Slabon')[['Faltante', 'Sobrante']].sum().reset_index()\n",
    "\n",
    "\n",
    "df_resultado['Material'] = df_resultado['Material'].astype(str).str.strip().str.lower()\n",
    "Resumen_AP[Resumen_AP.columns[0]] = Resumen_AP[Resumen_AP.columns[0]].astype(str).str.strip().str.lower()\n",
    "Resumen_AGP[Resumen_AGP.columns[0]] = Resumen_AGP[Resumen_AGP.columns[0]].astype(str).str.strip().str.lower()\n",
    "\n",
    "merged_df = pd.merge(df_resultado, Resumen_AP, left_on='Material', right_on=Resumen_AP.columns[0], how='left')\n",
    "merged_df.drop(Resumen_AP.columns[0], axis=1, inplace=True)\n",
    "merged_df.rename(columns={'Faltante': 'Faltante AP', 'Sobrante': 'Sobrante AP'}, inplace=True)\n",
    "# Reemplazando NaN por 0\n",
    "merged_df['Faltante AP'] = merged_df['Faltante AP'].fillna(0)\n",
    "merged_df['Sobrante AP'] = merged_df['Sobrante AP'].fillna(0)\n",
    "\n",
    "# Segundo merge: Uniendo merged_df con Resumen_AGP\n",
    "merged_df2 = pd.merge(merged_df, Resumen_AGP, left_on='Material', right_on=Resumen_AGP.columns[0], how='left')\n",
    "merged_df2.drop(Resumen_AGP.columns[0], axis=1, inplace=True)\n",
    "merged_df2.rename(columns={'Faltante': 'Faltante AGP', 'Sobrante': 'Sobrante AGP'}, inplace=True)\n",
    "# Reemplazando NaN por 0\n",
    "merged_df2['Faltante AGP'] = merged_df2['Faltante AGP'].fillna(0)\n",
    "merged_df2['Sobrante AGP'] = merged_df2['Sobrante AGP'].fillna(0)\n",
    "\n",
    "\n",
    "merged_df2['Faltantes'] = merged_df2['Faltante AP'] + merged_df2['Faltante AGP']\n",
    "merged_df2['Sobrantes'] = merged_df2['Sobrante AP'] + merged_df2['Sobrante AGP']\n",
    "\n",
    "columnas_existentes = [col for col in merged_df2.columns if col not in ['Material', 'Faltante AP', 'Faltante AGP', 'Sobrante AP', 'Sobrante AGP', 'Faltantes', 'Sobrantes']]\n",
    "column_order = ['Material'] + columnas_existentes + ['Faltante AP', 'Faltante AGP', 'Faltantes', 'Sobrante AP', 'Sobrante AGP', 'Sobrantes']\n",
    "\n",
    "\n",
    "merged_df2 = merged_df2[column_order]\n",
    "\n",
    "#PLANOGRAMA ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "# Obteniendo la primera columna de Resumen_AP y Resumen_AGP\n",
    "primer_columna_AP = Resumen_AP.columns[0]\n",
    "primer_columna_AGP = Resumen_AGP.columns[0]\n",
    "\n",
    "# Creando la columna 'AP' en Merged_df2\n",
    "merged_df2['AP'] = merged_df2['Material'].isin(Resumen_AP[primer_columna_AP]).astype(int)\n",
    "\n",
    "# Creando la columna 'AGP' en Merged_df2\n",
    "merged_df2['AGP'] = merged_df2['Material'].isin(Resumen_AGP[primer_columna_AGP]).astype(int)\n",
    "merged_df2['Total Planograma'] = ((merged_df2['AGP'] + merged_df2['AP']) != 0).astype(int)\n",
    "\n",
    "#CRUCE STOCK TIENDAS ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el DataFrame desde el archivo Excel\n",
    "df123 = pd.read_excel(ruta_origen_stock, sheet_name=\"Sheet1\", usecols=['Material', 'Libre utilización', 'Almacén'])\n",
    "\n",
    "# Convertir 'Material' a entero (si es posible), luego a cadena para eliminar '.0'\n",
    "df123['Material'] = pd.to_numeric(df123['Material'], errors='coerce').fillna(0).astype(int).astype(str)\n",
    "\n",
    "# Convertir 'Libre utilización' y 'Almacén' a numérico\n",
    "df123['Libre utilización'] = pd.to_numeric(df123['Libre utilización'], errors='coerce')\n",
    "df123['Almacén'] = pd.to_numeric(df123['Almacén'], errors='coerce')\n",
    "\n",
    "# Filtrar por 'Almacén' para quedarse con 1100.0 y NaN\n",
    "filtered_df = df123[(df123['Almacén'] == 1100.0) | (df123['Almacén'].isna())]\n",
    "\n",
    "# Realizar el cruce (merge) entre los DataFrames\n",
    "TiendaUE = pd.merge(filtered_df, df_cod_actual, left_on='Material', right_on='Nro_pieza_fabricante_1', how='left')\n",
    "\n",
    "# Rellenar los NaN en 'Cod_Actual_1' con los valores de 'Material' original\n",
    "TiendaUE['Cod_Actual_1'] = TiendaUE['Cod_Actual_1'].fillna(TiendaUE['Material'])\n",
    "\n",
    "# Eliminar la columna 'Material' y renombrar 'Cod_Actual_1' a 'Material'\n",
    "TiendaUE = TiendaUE.drop('Material', axis=1)\n",
    "TiendaUE.rename(columns={'Cod_Actual_1': 'Material'}, inplace=True)\n",
    "\n",
    "# Agrupar por 'Material' y sumar 'Libre utilización'\n",
    "df_stock = TiendaUE.groupby('Material')['Libre utilización'].sum().reset_index()\n",
    "\n",
    "# Ajustar valores menores a 1 a 0\n",
    "df_stock['Libre utilización'] = df_stock['Libre utilización'].apply(lambda x: 0 if x < 1 else x)\n",
    "\n",
    "# Convertir 'Libre utilización' a entero para eliminar decimales no necesarios\n",
    "df_stock['Libre utilización'] = df_stock['Libre utilización'].astype(int)\n",
    "\n",
    "duplicados = df_stock['Material'].duplicated()\n",
    "\n",
    "# Para ver si hay al menos un duplicado en esa columna\n",
    "hay_duplicados = duplicados.any()\n",
    "print(\"¿Hay duplicados?:\", hay_duplicados)\n",
    "\n",
    "# Para contar el número total de duplicados en esa columna\n",
    "numero_duplicados = duplicados.sum()\n",
    "print(\"Número de duplicados:\", numero_duplicados)\n",
    "\n",
    "# Para obtener un DataFrame con todos los duplicados en esa columna\n",
    "df_duplicados = df_stock[df_stock['Material'].duplicated(keep=False)]\n",
    "print(df_duplicados)\n",
    "\n",
    "df_stock.rename(columns={'Libre utilización': 'Stock Tiendas'}, inplace=True)\n",
    "df_stock['Material'] = df_stock['Material'].fillna(0).astype(int)\n",
    "df_stock['Stock Tiendas'] = df_stock['Stock Tiendas'].fillna(0).astype(int)\n",
    "\n",
    "#STOCK CD ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Realizar el cruce (merge) entre los DataFrames\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Especifica las columnas que necesitas\n",
    "# cols_to_use3 = ['Material', 'Libre utilización', 'Alm.', 'Ce.', 'Insp. Calidad', 'Traslado', 'Ult. Eslabon']\n",
    "cols_to_use3 = ['Ult. Eslabon', 'Libre utilización', 'Almacén', 'Centro', 'Inspecc.de calidad', 'Trans./Trasl.']\n",
    "# Convertir la columna 'Material' a tipo object (string) para evitar el .0\n",
    "# df_filtered1.rename(columns={'Inspecc.de calidad': 'Material'}, inplace=True)\n",
    "\n",
    "# Carga solo las columnas que necesitas desde la hoja \"Sheet1\"\n",
    "dfleerstock = pd.read_excel(ruta_origen_stock2, sheet_name='Sheet1', usecols=cols_to_use3)\n",
    "\n",
    "# Convierte 'Alm.' a entero, maneja NaN y luego convierte a cadena\n",
    "# dfleerstock['Alm.'] = pd.to_numeric(dfleerstock['Alm.'], errors='coerce').fillna(0).astype(int).astype(str)\n",
    "dfleerstock['Almacén'] = pd.to_numeric(dfleerstock['Almacén'], errors='coerce').fillna(0).astype(int).astype(str)\n",
    "\n",
    "# Convierte 'Ce.' a cadena\n",
    "dfleerstock['Centro'] = dfleerstock['Centro'].astype(str)\n",
    "# Convertir 'Ce.' a cadena y eliminar '.0'\n",
    "dfleerstock['Centro'] = dfleerstock['Centro'].astype(str).str.replace('.0', '', regex=False)\n",
    "\n",
    "\n",
    "# Define los valores permitidos para 'Alm.' y 'Ce.'\n",
    "# allowed_alm = ['1100', '1600']\n",
    "allowed_centers = ['710', '712', '713', '714', '0714', '0710', '0712', '0713']\n",
    "print(\"Valores únicos en Alm.:\", dfleerstock['Almacén'].unique())\n",
    "print(\"Valores únicos en Ce.:\", dfleerstock['Centro'].unique())\n",
    "\n",
    "\n",
    "\n",
    "df_filtered1 = dfleerstock[\n",
    "    (dfleerstock['Centro'].isin(allowed_centers))\n",
    "].copy()\n",
    "dfleerstock2 = pd.read_excel(ruta_origen_stock2, sheet_name='Sheet1')\n",
    "dfleerstock2.head(3)\n",
    "\n",
    "df_filtered1.rename(columns={'Ult. Eslabon': 'Material'}, inplace=True)\n",
    "df_filtered1.head()\n",
    "FilteredUE = pd.merge(df_filtered1, df_cod_actual, left_on='Material', right_on='Nro_pieza_fabricante_1', how='left')\n",
    "\n",
    "\n",
    "# Rellenar los NaN en 'Cod_Actual_1' con los valores de 'Material' original\n",
    "FilteredUE['Cod_Actual_1'] = FilteredUE['Cod_Actual_1'].fillna(FilteredUE['Material'])\n",
    "# Eliminar la columna 'Material' y renombrar 'Cod_Actual_1' a 'Material'\n",
    "FilteredUE = FilteredUE.drop('Material', axis=1)\n",
    "\n",
    "\n",
    "# FilteredUE = FilteredUE.drop('Ult. Eslabon', axis=1)\n",
    "FilteredUE.head()\n",
    "\n",
    "FilteredUE.rename(columns={'Inspecc.de calidad': 'Insp. Calidad', 'Trans./Trasl.': 'Traslado'}, inplace=True)\n",
    "\n",
    "FilteredUE.rename(columns={'Cod_Actual_1': 'Ult. Eslabon'}, inplace=True)\n",
    "# Aquí se ajustan los valores de 'Libre utilización' menores a 0 a 0\n",
    "FilteredUE['Libre utilización'] = FilteredUE['Libre utilización'].apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "# Verifica si hay algún registro después del filtrado\n",
    "if FilteredUE.empty:\n",
    "    print(\"No hay registros después del filtrado. Verifique los criterios de filtrado y los datos de entrada.\")\n",
    "else:\n",
    "    df_stock4 = FilteredUE.groupby(['Ult. Eslabon']).agg({\n",
    "        'Libre utilización': 'sum',\n",
    "        'Insp. Calidad': 'sum',\n",
    "        'Traslado': 'sum'\n",
    "    }).reset_index()\n",
    "df_stock4['Ult. Eslabon'] = df_stock4['Ult. Eslabon'].astype(str)\n",
    "df_stock4['Libre utilización'] = df_stock4['Libre utilización'].fillna(0).astype(int)\n",
    "df_stock4['Traslado'] = df_stock4['Traslado'].fillna(0).astype(int) \n",
    "df_stock4['Insp. Calidad'] = df_stock4['Insp. Calidad'].fillna(0).astype(int)\n",
    "#Agregar una columna 'Control' sumando 'Insp. Calidad' y 'Traslado'\n",
    "df_stock4['Control'] = df_stock4['Insp. Calidad'] + df_stock4['Traslado']\n",
    "df_stock4.rename(columns={'Ult. Eslabon': 'Material'}, inplace=True)\n",
    "FilteredUE = FilteredUE.rename(columns={'Ult. Eslabon': 'Material'})\n",
    "\n",
    "duplicados = df_stock4['Material'].duplicated()\n",
    "\n",
    "# Para ver si hay al menos un duplicado en esa columna\n",
    "hay_duplicados = duplicados.any()\n",
    "print(\"¿Hay duplicados?:\", hay_duplicados)\n",
    "\n",
    "# Para contar el número total de duplicados en esa columna\n",
    "numero_duplicados = duplicados.sum()\n",
    "print(\"Número de duplicados:\", numero_duplicados)\n",
    "\n",
    "\n",
    "df_duplicados = df_stock4[df_stock4['Material'].duplicated(keep=False)]\n",
    "print(df_duplicados)\n",
    "df_stock4 = df_stock4.drop_duplicates(keep='first')\n",
    "\n",
    "duplicados = FilteredUE['Material'].duplicated()\n",
    "\n",
    "# Para ver si hay al menos un duplicado en esa columna\n",
    "hay_duplicados = duplicados.any()\n",
    "print(\"¿Hay duplicados?:\", hay_duplicados)\n",
    "\n",
    "# Para contar el número total de duplicados en esa columna\n",
    "numero_duplicados = duplicados.sum()\n",
    "print(\"Número de duplicados:\", numero_duplicados)\n",
    "\n",
    "# Para obtener un DataFrame con todos los duplicados en esa columna\n",
    "df_duplicados = FilteredUE[FilteredUE['Material'].duplicated(keep=False)]\n",
    "print(df_duplicados)\n",
    "\n",
    "#STOCK 711 ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "# Carga solo las columnas que necesitas desde la hoja \"Sheet1\"\n",
    "# Especifica las columnas que necesitas\n",
    "cols_to_use4 = ['Material', 'Libre utilización', 'Almacén', 'Centro', 'Inspecc.de calidad', 'Trans./Trasl.', 'Ult. Eslabon']\n",
    "\n",
    "# Carga solo las columnas que necesitas desde la hoja \"Sheet1\"\n",
    "dfleerstock2 = pd.read_excel(ruta_origen_stock2, sheet_name='Sheet1', usecols=cols_to_use4)\n",
    "dfleerstock2.rename(columns={'Almacén': 'Alm.', 'Centro': 'Ce.', 'Inspecc.de calidad': 'Insp. Calidad', 'Trans./Trasl.': 'Traslado'}, inplace=True)\n",
    "dfleerstock.rename(columns={'Centro': 'Ce.'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Convierte 'Alm.' y 'Ce.' a cadena para garantizar la consistencia en el filtrado\n",
    "dfleerstock2['Alm.'] = dfleerstock2['Alm.'].astype(str)\n",
    "dfleerstock2['Ce.'] = dfleerstock2['Ce.'].astype(str)\n",
    "dfleerstock['Ce.'] = dfleerstock['Ce.'].astype(str).str.replace('.0', '', regex=False)\n",
    "\n",
    "# Filtrar por 'Centro'\n",
    "allowed_centers2 = ['0711', '711', '711.0']\n",
    "df_filtered2 = dfleerstock2[dfleerstock2['Ce.'].isin(allowed_centers2)]\n",
    " # Convertir la columna 'Material' a tipo object (string) para evitar el .0\n",
    "FilteredUE711 = pd.merge(df_filtered2, df_cod_actual, left_on='Material', right_on='Nro_pieza_fabricante_1', how='left')\n",
    "FilteredUE711['Cod_Actual_1'] = FilteredUE711['Cod_Actual_1'].fillna(FilteredUE711['Material'])\n",
    "\n",
    "# Eliminar la columna 'Material' y renombrar 'Cod_Actual_1' a 'Material'\n",
    "FilteredUE711 = FilteredUE711.drop('Material', axis=1)\n",
    "FilteredUE711 = FilteredUE711.drop('Ult. Eslabon', axis=1)\n",
    "FilteredUE711.rename(columns={'Cod_Actual_1': 'Ult. Eslabon'}, inplace=True)\n",
    "\n",
    "# Verifica si hay algún registro después del filtrado\n",
    "if FilteredUE711.empty:\n",
    "    print(\"No hay registros después del filtrado. Verifique los criterios de filtrado y los datos de entrada.\")\n",
    "else:\n",
    "    # Agrupar por 'Material' y sumar las columnas especificadas\n",
    "    FilteredUE711 = FilteredUE711.groupby('Ult. Eslabon').agg({\n",
    "        'Libre utilización': 'sum',\n",
    "        'Insp. Calidad': 'sum',\n",
    "        'Traslado': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Convertir la columna 'Material' a tipo object (string) para evitar el .0\n",
    "    FilteredUE711['Ult. Eslabon'] = FilteredUE711['Ult. Eslabon'].astype(str)\n",
    "\n",
    "    # Añadir la columna 'Control' que es la suma de 'Insp. Calidad' y 'Traslado'\n",
    "    FilteredUE711['Control'] = FilteredUE711['Insp. Calidad'] + FilteredUE711['Traslado']\n",
    "\n",
    "    # Asumiendo que deseas rellenar los NaN con 0 y convertir a entero\n",
    "    FilteredUE711['Libre utilización'] = FilteredUE711['Libre utilización'].fillna(0).astype(int)\n",
    "    FilteredUE711['Insp. Calidad'] = FilteredUE711['Insp. Calidad'].fillna(0).astype(int)\n",
    "    FilteredUE711['Traslado'] = FilteredUE711['Traslado'].fillna(0).astype(int)\n",
    "    FilteredUE711['Control'] = FilteredUE711['Control'].fillna(0).astype(int)\n",
    "    FilteredUE711.rename(columns={'Ult. Eslabon': 'Material'}, inplace=True)\n",
    "\n",
    "    FilteredUE711.rename(columns={'Libre utilización': 'Stock 711'}, inplace=True)\n",
    "\n",
    "#MERGE STOCK TIENDA ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "df_stock['Material'] = df_stock['Material'].astype(str)\n",
    "merged_df2 = pd.merge(merged_df2, df_stock[['Material', 'Stock Tiendas']], on='Material', how='left')\n",
    "\n",
    "merged_df2 = pd.merge(merged_df2, df_stock4[['Material', 'Libre utilización', 'Control']], on='Material', how='left')\n",
    "\n",
    "# Llenar NaN con 0 en la columna 'Libre utilización'\n",
    "merged_df2['Libre utilización'] = merged_df2['Libre utilización'].fillna(0)\n",
    "# Convertir 'Material' y 'Stock Tiendas' a enteros, asegurándose de que no haya NaNs\n",
    "merged_df2.rename(columns={'Libre utilización': 'Stock CD'}, inplace=True)\n",
    "merged_df2['Stock Tiendas'] = merged_df2['Stock Tiendas'].fillna(0)\n",
    "\n",
    "# Hacer un merge \n",
    "merged_df2 = pd.merge(merged_df2, FilteredUE711[['Material', 'Stock 711']], on='Material', how='left')\n",
    "\n",
    "merged_df2['Stock 711'] = merged_df2['Stock 711'].fillna(0)\n",
    "merged_df2['Control'] = merged_df2['Control'].fillna(0)\n",
    "merged_df2.head()\n",
    "\n",
    "# MANIPULAR FC ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Obtener los últimos dos dígitos del año actual y del siguiente año\n",
    "año_actual = datetime.now().year\n",
    "ultimo_digito_año_actual = str(año_actual)[-2:]\n",
    "ultimo_digito_año_siguiente = str(año_actual + 1)[-2:]\n",
    "\n",
    "# Leer el archivo Excel\n",
    "xlinbound = pd.ExcelFile(ruta_completa_archivo)\n",
    "\n",
    "# Buscar nombres de hojas que contengan la palabra 'Inbound'\n",
    "hojas_inbound = [hoja for hoja in xlinbound.sheet_names if 'Inbound' in hoja]\n",
    "\n",
    "# Leer la hoja de Excel que contiene 'Inbound' en el nombre, empezando por la segunda fila\n",
    "if hojas_inbound:\n",
    "    # El parámetro 'header=1' indica que la segunda fila contiene los nombres de las columnas\n",
    "    inbound_dataframe = pd.read_excel(ruta_completa_archivo, sheet_name=hojas_inbound[0], header=1)\n",
    "    \n",
    "    # Filtrar columnas que contengan los últimos dos dígitos del año actual o del siguiente en el nombre\n",
    "    columnas_filtradas = [col for col in inbound_dataframe.columns if ultimo_digito_año_actual in col or ultimo_digito_año_siguiente in col]\n",
    "    inbound_dataframe_filtrado = inbound_dataframe[columnas_filtradas + ['Ult. Eslabón']]\n",
    "    \n",
    "    # Convertir las columnas pertinentes a strings para el merge\n",
    "    merged_df2['Material'] = merged_df2['Material'].astype(str)\n",
    "    inbound_dataframe_filtrado = inbound_dataframe_filtrado.copy()\n",
    "    inbound_dataframe_filtrado['Ult. Eslabón'] = inbound_dataframe_filtrado['Ult. Eslabón'].astype(str)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "month_to_number = {\n",
    "    'ene': 1, 'feb': 2, 'mar': 3, 'abr': 4, 'may': 5, 'jun': 6,\n",
    "    'jul': 7, 'ago': 8, 'sept': 9, 'oct': 10, 'nov': 11, 'dic': 12\n",
    "}\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Crear un nuevo DataFrame para los valores semanales\n",
    "weekly_values_df = pd.DataFrame()\n",
    "\n",
    "for column in inbound_dataframe_filtrado.columns:\n",
    "    if column == 'Ult. Eslabón':\n",
    "        continue\n",
    "\n",
    "    mes, año = column.split('-')\n",
    "    mes_numero = month_to_number[mes.lower()]\n",
    "    año_numero = int('20' + año)\n",
    "\n",
    "    valor_mensual = inbound_dataframe_filtrado[column].astype(float)\n",
    "\n",
    "    # Dividir el valor mensual por el número aproximado de semanas\n",
    "    valor_semanal = valor_mensual / 4.33\n",
    "\n",
    "    # Determinar el rango de fechas de la semana para el mes\n",
    "    primera_fecha = datetime(año_numero, mes_numero, 1)\n",
    "    ultima_fecha = primera_fecha + pd.offsets.MonthEnd()\n",
    "\n",
    "    # Contar los días que pertenecen a cada mes\n",
    "    dias_en_semana = Counter()\n",
    "    fecha_actual = primera_fecha\n",
    "    while fecha_actual <= ultima_fecha:\n",
    "        semana_del_año = fecha_actual.isocalendar()[1]\n",
    "        dias_en_semana[semana_del_año] += 1\n",
    "        fecha_actual += timedelta(days=1)\n",
    "\n",
    "    # Asignar semanas a los meses correspondientes\n",
    "    for semana, dias in dias_en_semana.items():\n",
    "        # Determinar si la semana se debe asignar a este mes\n",
    "        if dias >= 4:  # Si al menos 4 días de la semana caen en este mes, asignar la semana a este mes\n",
    "            nombre_columna_semana = f'Sem {semana} ({año})'\n",
    "            weekly_values_df[nombre_columna_semana] = valor_semanal\n",
    "\n",
    "# Agregar la columna 'Ult. Eslabón' al nuevo DataFrame, asegurándote de que el índice coincida\n",
    "weekly_values_df['Ult. Eslabón'] = inbound_dataframe_filtrado['Ult. Eslabón'].values\n",
    "weekly_values_df.rename(columns={'Ult. Eslabón': 'Material'}, inplace=True)\n",
    "merged_df2.fillna(0, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSITO ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Leer la hoja que se llama 'TR Final' en un DataFrame.\n",
    "df_transito_original = pd.read_excel(ruta_origen_stock3, sheet_name='Sheet1')\n",
    "\n",
    "\n",
    "# Normalizar los nombres de las columnas (en caso de que haya inconsistencias en mayúsculas y minúsculas).\n",
    "# Normalizar los nombres de las columnas (en caso de que haya inconsistencias en mayúsculas y minúsculas).\n",
    "df_transito_original.columns = [col.title() if isinstance(col, str) else col for col in df_transito_original.columns]\n",
    "df_transito_original['Fecha'] = pd.to_datetime(df_transito_original['Fecha'])\n",
    "\n",
    "\n",
    "# Verificar que las columnas existan\n",
    "columnas_requeridas = ['Material', 'Cantidad', 'Fecha']\n",
    "for col in columnas_requeridas:\n",
    "    if col not in df_transito_original.columns:\n",
    "        raise ValueError(f\"La columna requerida '{col}' no está en el DataFrame.\")\n",
    "    \n",
    "    \n",
    "# Convertir todas las columnas a formato string (alfanumérico) excepto la columna de fecha\n",
    "for column in df_transito_original.columns:\n",
    "    if column != 'Fecha':  # Excluyendo la columna de fecha\n",
    "        df_transito_original[column] = df_transito_original[column].astype(str)\n",
    "\n",
    "# Convertir la columna de fecha al formato correcto de fecha y hora\n",
    "df_transito_original['Fecha'] = pd.to_datetime(df_transito_original['Fecha'], errors='coerce')  # 'coerce' maneja los errores\n",
    "\n",
    "\n",
    "# Crear un nuevo DataFrame con 'Material' como índice y las combinaciones de 'Semanas' y 'Año' como columnas.\n",
    "# Primero'Semana_Año'.\n",
    "df_transito_original['Semana_Año'] = df_transito_original.apply(lambda x: str(x['Semana']) + '-' + str(x['Año']), axis=1)\n",
    "\n",
    "\n",
    "# Función para obtener la semana correspondiente al mes de la fecha\n",
    "def asignar_semana_al_mes(fecha):\n",
    "    # Encuentra el primer día del mes de la fecha dada\n",
    "    primer_dia_del_mes = fecha.replace(day=1)\n",
    "    # Encuentra el último día de la semana en que cae el primer día del mes\n",
    "    ultimo_dia_semana = primer_dia_del_mes + timedelta(days=6 - primer_dia_del_mes.weekday())\n",
    "    # Si la fecha dada es mayor al último día de la primera semana completa del mes, usar la fecha dada\n",
    "    # De lo contrario, usa el primer día del mes\n",
    "    fecha_referencia = fecha if fecha > ultimo_dia_semana else primer_dia_del_mes\n",
    "    # Encuentra el año y la semana de la fecha de referencia\n",
    "    año, semana, dia_semana = fecha_referencia.isocalendar()\n",
    "    return semana, año\n",
    "\n",
    "# Aplicar la función a la columna 'Fecha' para obtener la nueva semana y año\n",
    "df_transito_original['Semana_Asignada'], df_transito_original['Año_Asignado'] = zip(*df_transito_original['Fecha'].apply(asignar_semana_al_mes))\n",
    "\n",
    "\n",
    "# Crear la columna 'Semana_Año_Asignada'\n",
    "df_transito_original['Semana_Año_Asignada'] = df_transito_original['Semana_Asignada'].astype(str) + '-' + df_transito_original['Año_Asignado'].astype(str)\n",
    "\n",
    "df_transito_original['Cantidad'] = pd.to_numeric(df_transito_original['Cantidad'], errors='coerce')\n",
    "\n",
    "df_transito = df_transito_original.pivot_table(\n",
    "    index='Material',\n",
    "    columns='Semana_Año_Asignada',\n",
    "    values='Cantidad',\n",
    "    aggfunc='sum',  # Sumará los valores numéricos\n",
    "    fill_value=0  # Llena con ceros si no hay valores\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "# Renombrar las columnas para que sigan el formato 'Sem X (YY)'\n",
    "nuevos_nombres_columnas = {col: f\"Sem {int(col.split('-')[0])} ({col.split('-')[1][2:]})\" for col in df_transito.columns if '-' in col}\n",
    "df_transito.rename(columns=nuevos_nombres_columnas, inplace=True)\n",
    "inbound_dataframe_filtrado.rename(columns={'Ult. Eslabón': 'Material'}, inplace=True)\n",
    "\n",
    "df_transito['Material'] = df_transito['Material'].astype(str)\n",
    "\n",
    "# Ahora puedes realizar la conversión a string sin la advertencia\n",
    "inbound_dataframe_filtrado['Material'] = inbound_dataframe_filtrado['Material'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Material', 'Sem 11 (24)', 'Sem 12 (24)', 'Sem 13 (24)', 'Sem 14 (24)',\n",
       "       'Sem 15 (24)', 'Sem 16 (24)', 'Sem 17 (24)', 'Sem 18 (24)',\n",
       "       'Sem 19 (24)', 'Sem 20 (24)', 'Sem 21 (24)', 'Sem 22 (24)',\n",
       "       'Sem 23 (24)', 'Sem 24 (24)', 'Sem 25 (24)', 'Sem 26 (24)',\n",
       "       'Sem 27 (24)', 'Sem 28 (24)', 'Sem 29 (24)', 'Sem 30 (24)',\n",
       "       'Sem 31 (24)', 'Sem 32 (24)', 'Sem 33 (24)', 'Sem 34 (24)',\n",
       "       'Sem 35 (24)', 'Sem 36 (24)', 'Sem 37 (24)', 'Sem 38 (24)',\n",
       "       'Sem 39 (24)', 'Sem 40 (24)', 'Sem 41 (24)', 'Sem 43 (24)',\n",
       "       'Sem 44 (24)', 'Sem 45 (24)', 'Sem 48 (24)'],\n",
       "      dtype='object', name='Semana_Año_Asignada')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transito.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "# Obtener la fecha actual\n",
    "fecha_actual = datetime.datetime.now()\n",
    "# Establecer la fecha de referencia en el día 15 del mes actual\n",
    "fecha_referencia = fecha_actual.replace(day=15)\n",
    "\n",
    "# Encontrar el número de semana del año para el día 15 del mes\n",
    "numero_semana_quincena = fecha_referencia.isocalendar()[1]\n",
    "\n",
    "nombre_columna_semana = f\"Sem {numero_semana_quincena} ({fecha_actual.strftime('%y')})\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sem 11 (24)\n"
     ]
    }
   ],
   "source": [
    "merged_df2[nombre_columna_semana] = np.nan  # Agregar la nueva columna con el nombre de la semana calculada\n",
    "# todo numerico\n",
    "merged_df2['Stock Tiendas'] = pd.to_numeric(merged_df2['Stock Tiendas'], errors='coerce').fillna(0)\n",
    "merged_df2['Stock CD'] = pd.to_numeric(merged_df2['Stock CD'], errors='coerce').fillna(0)\n",
    "merged_df2['Faltante AP'] = pd.to_numeric(merged_df2['Faltante AP'], errors='coerce').fillna(0)\n",
    "merged_df2['Faltantes'] = pd.to_numeric(merged_df2['Faltantes'], errors='coerce').fillna(0)\n",
    "\n",
    "# Realizar el cálculo y asignarlo a la nueva columna\n",
    "merged_df2[nombre_columna_semana] = (merged_df2['Stock Tiendas'] + merged_df2['Stock CD']) - (merged_df2['Faltantes'])\n",
    "print(nombre_columna_semana)\n",
    "\n",
    "merged_df2['Material'] = merged_df2['Material'].astype(str)\n",
    "df_transito['Material'] = df_transito['Material'].astype(str)\n",
    "weekly_values_df['Material'] = weekly_values_df['Material'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semana_actual =numero_semana_quincena\n",
    "semana_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sem 12 (24)\n",
      "Sem 13 (24)\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# fecha_referencia = datetime.now().replace(day=1)\n",
    "# numero_semana_quincena = fecha_referencia.isocalendar()[1]\n",
    "\n",
    "# Calcular la semana siguiente y ajustar el año si es necesario\n",
    "semana_siguiente = semana_actual + 1\n",
    "año_actual = fecha_referencia.year\n",
    "\n",
    "if semana_siguiente > 52:\n",
    "    semana_siguiente = 1\n",
    "    año_actual += 1\n",
    "\n",
    "# Crear el nombre de la columna para la semana siguiente\n",
    "nombre_columna_semana_siguiente = f\"Sem {semana_siguiente} ({str(año_actual)[-2:]})\"\n",
    "\n",
    "# Inicializar la columna de la semana siguiente en merged_df4\n",
    "merged_df2[nombre_columna_semana_siguiente] = merged_df2[nombre_columna_semana]\n",
    "\n",
    "if nombre_columna_semana in merged_df2.columns:\n",
    "    merged_df2[nombre_columna_semana_siguiente] = pd.to_numeric(merged_df2[nombre_columna_semana], errors='coerce').fillna(0)\n",
    "\n",
    "# Asegurarse de que la columna 'Control' es numérica\n",
    "merged_df2['Control'] = pd.to_numeric(merged_df2['Control'], errors='coerce').fillna(0)\n",
    "\n",
    "# Sumar los valores de la columna 'Control' a la nueva columna de la semana siguiente\n",
    "merged_df2[nombre_columna_semana_siguiente] += merged_df2['Control']\n",
    "\n",
    "merged_df2['Material'] = merged_df2['Material'].astype(str)\n",
    "df_transito['Material'] = df_transito['Material'].astype(str)\n",
    "weekly_values_df['Material'] = weekly_values_df['Material'].astype(str)\n",
    "\n",
    "merged_df2 = merged_df2.merge(df_transito[['Material', nombre_columna_semana_siguiente]], on='Material', how='left', suffixes=('', '_transito'))\n",
    "merged_df2[nombre_columna_semana_siguiente] = merged_df2[nombre_columna_semana_siguiente] + merged_df2[nombre_columna_semana_siguiente + '_transito'].fillna(0)\n",
    "\n",
    "# Combinación (merge) con weekly_values_df\n",
    "merged_df2 = merged_df2.merge(weekly_values_df[['Material', nombre_columna_semana_siguiente]], on='Material', how='left', suffixes=('', '_weekly'))\n",
    "\n",
    "# Condición para realizar la resta solo si el valor después de sumar con transito es mayor a 0\n",
    "merged_df2[nombre_columna_semana_siguiente] = merged_df2.apply(\n",
    "    lambda row: row[nombre_columna_semana_siguiente] - row[nombre_columna_semana_siguiente + '_weekly']\n",
    "    if row[nombre_columna_semana_siguiente] > 0 else row[nombre_columna_semana_siguiente], axis=1\n",
    ").fillna(0)\n",
    "\n",
    "# Eliminar las columnas auxiliares del merge\n",
    "merged_df2.drop(columns=[nombre_columna_semana_siguiente + '_transito', nombre_columna_semana_siguiente + '_weekly'], inplace=True)\n",
    "\n",
    "# Imprimir el DataFrame para verificar los resultados\n",
    "print(nombre_columna_semana_siguiente)\n",
    "\n",
    "#TERCERA SEMANA ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Convierte todo a strng\n",
    "merged_df2['Material'] = merged_df2['Material'].astype(str)\n",
    "df_transito['Material'] = df_transito['Material'].astype(str)\n",
    "weekly_values_df['Material'] = weekly_values_df['Material'].astype(str)\n",
    "\n",
    "semana_actual = int(nombre_columna_semana_siguiente.split(' ')[1].split('(')[0])\n",
    "año_actual = int(nombre_columna_semana_siguiente.split('(')[1].split(')')[0])\n",
    "semana_nueva = semana_actual + 1\n",
    "año_nuevo = año_actual\n",
    "if semana_nueva > 52:\n",
    "    semana_nueva = 1\n",
    "    año_nuevo += 1\n",
    "nombre_columna_semana_nueva = f\"Sem {semana_nueva} ({str(año_nuevo)[-2:]})\"\n",
    "\n",
    "# Inicializa la columna de la nueva semana en merged_df4.\n",
    "merged_df2[nombre_columna_semana_nueva] = merged_df2[nombre_columna_semana_siguiente]\n",
    "\n",
    "# Asegúrate de que la columna 'Stock 711' es numérica.\n",
    "merged_df2['Stock 711'] = pd.to_numeric(merged_df2['Stock 711'], errors='coerce').fillna(0)\n",
    "\n",
    "# Suma los valores de la columna 'Stock 711' a la nueva columna de la semana.\n",
    "merged_df2[nombre_columna_semana_nueva] += merged_df2['Stock 711']\n",
    "\n",
    "# Realiza el merge con 'df_transito' y suma los valores correspondientes a la nueva semana.\n",
    "merged_df2 = merged_df2.merge(\n",
    "    df_transito[['Material', nombre_columna_semana_nueva]],\n",
    "    on='Material',\n",
    "    how='left',\n",
    "    suffixes=('', '_transito_nueva')\n",
    ")\n",
    "merged_df2[nombre_columna_semana_nueva] += merged_df2[nombre_columna_semana_nueva + '_transito_nueva'].fillna(0)\n",
    "\n",
    "# Merge con el forecast\n",
    "merged_df2 = merged_df2.merge(\n",
    "    weekly_values_df[['Material', nombre_columna_semana_nueva]],\n",
    "    on='Material',\n",
    "    how='left',\n",
    "    suffixes=('', '_forecast_nueva')\n",
    ")\n",
    "\n",
    "## Restar solo si el resultado del merge con 'df_transito' es igual o mayor a 0\n",
    "merged_df2[nombre_columna_semana_nueva] = merged_df2.apply(\n",
    "    lambda row: row[nombre_columna_semana_nueva] - row[nombre_columna_semana_nueva + '_forecast_nueva']\n",
    "    if row[nombre_columna_semana_nueva] > 0 else row[nombre_columna_semana_nueva], axis=1\n",
    ").fillna(0)\n",
    "\n",
    "# Eliminar las columnas auxiliares del merge\n",
    "merged_df2.drop(columns=[nombre_columna_semana_nueva + '_transito_nueva', nombre_columna_semana_nueva + '_forecast_nueva'], inplace=True)\n",
    "print(nombre_columna_semana_nueva)\n",
    "\n",
    "#46 SEMANAS ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "semana_actual = int(nombre_columna_semana_siguiente.split(' ')[1].split('(')[0])\n",
    "año_actual = int(nombre_columna_semana_siguiente.split('(')[1].split(')')[0])\n",
    "semana_siguiente = semana_actual + 1\n",
    "año_siguiente = año_actual\n",
    "if semana_siguiente > 52:\n",
    "    semana_siguiente = 1\n",
    "    año_siguiente += 1\n",
    "nombre_columna_semana_siguiente = f\"Sem {semana_siguiente} ({str(año_siguiente)[-2:]})\"\n",
    "\n",
    "for i in range(46):  # Repetir el proceso 46 veces\n",
    "    # Calcula el nombre de la columna de la nueva semana.\n",
    "    semana_actual = int(nombre_columna_semana_siguiente.split(' ')[1].split('(')[0])\n",
    "    año_actual = int(nombre_columna_semana_siguiente.split('(')[1].split(')')[0])\n",
    "    semana_nueva = semana_actual + 1\n",
    "    año_nuevo = año_actual\n",
    "    if semana_nueva > 52:\n",
    "        semana_nueva = 1\n",
    "        año_nuevo += 1\n",
    "    nombre_columna_semana_nueva = f\"Sem {semana_nueva} ({str(año_nuevo)[-2:]})\"\n",
    "\n",
    "    # Inicializa la columna de la nueva semana en merged_df4.\n",
    "    merged_df2[nombre_columna_semana_nueva] = merged_df2[nombre_columna_semana_siguiente]\n",
    "\n",
    "    # Realiza el merge con 'df_transito' y suma los valores correspondientes a la nueva semana.\n",
    "    if nombre_columna_semana_nueva in df_transito.columns:\n",
    "        merged_df2 = merged_df2.merge(\n",
    "            df_transito[['Material', nombre_columna_semana_nueva]],\n",
    "            on='Material',\n",
    "            how='left',\n",
    "            suffixes=('', '_transito_nueva')\n",
    "        )\n",
    "        merged_df2[nombre_columna_semana_nueva] += merged_df2[nombre_columna_semana_nueva + '_transito_nueva'].fillna(0)\n",
    "        # Elimina la columna auxiliar del merge\n",
    "        merged_df2.drop(columns=[nombre_columna_semana_nueva + '_transito_nueva'], inplace=True)\n",
    "\n",
    "        # Realiza el merge con 'weekly_values_df' y resta el forecast correspondiente a la nueva semana solo si el total es mayor a 0\n",
    "        if nombre_columna_semana_nueva in weekly_values_df.columns:\n",
    "            merged_df2 = merged_df2.merge(\n",
    "                weekly_values_df[['Material', nombre_columna_semana_nueva]],\n",
    "                on='Material',\n",
    "                how='left',\n",
    "                suffixes=('', '_forecast_nueva')\n",
    "            )\n",
    "            merged_df2[nombre_columna_semana_nueva] = merged_df2.apply(\n",
    "                lambda row: max(row[nombre_columna_semana_nueva] - row[nombre_columna_semana_nueva + '_forecast_nueva'], 0)\n",
    "                if row[nombre_columna_semana_nueva] > 0 else row[nombre_columna_semana_nueva], axis=1\n",
    "            ).fillna(0)\n",
    "            merged_df2.drop(columns=[nombre_columna_semana_nueva + '_forecast_nueva'], inplace=True)\n",
    "\n",
    "    # Actualiza el nombre de la columna para la próxima iteración\n",
    "    nombre_columna_semana_siguiente = nombre_columna_semana_nueva\n",
    "\n",
    "    \n",
    "\n",
    "#PORCENTAJES DE DISPONIBILIDAD\n",
    "\n",
    "columnas_semanas = [col for col in merged_df2.columns if col.startswith(\"Sem \") and \"D\" not in col]\n",
    "columnas_semanas_comunes = [col for col in columnas_semanas if col in weekly_values_df.columns]\n",
    "\n",
    "# # Realizar un merge (fusión) para alinear las filas según el 'Material'\n",
    "# merged_df2 = merged_df2.merge(weekly_values_df[['Material'] + columnas_semanas], on='Material', how='left', suffixes=('', '_weekly'))\n",
    "# Realizar un merge (fusión) para alinear las filas según el 'Material', solo con las columnas comunes\n",
    "merged_df2 = merged_df2.merge(weekly_values_df[['Material'] + columnas_semanas_comunes], on='Material', how='left', suffixes=('', '_weekly'))\n",
    "\n",
    "# Diccionario para almacenar las nuevas columnas\n",
    "nuevas_columnas = {}\n",
    "\n",
    "# Iterar sobre las columnas de semanas originales\n",
    "# for col in columnas_semanas:\n",
    "# Iterar sobre las columnas de semanas originales que se han comprobado que existen\n",
    "for col in columnas_semanas_comunes:\n",
    "    # Verificar si la columna de la semana existe en weekly_values_df\n",
    "    if col not in weekly_values_df.columns:\n",
    "        print(f\"No se encontró la columna: {col}\")\n",
    "        continue  # Continuar con la siguiente iteración\n",
    "\n",
    "    nueva_col = col + \"D\"  # Crear el nombre de la nueva columna añadiendo \"D\" al final\n",
    "\n",
    "    # Realizar el cálculo basado en las condiciones\n",
    "    condiciones = [\n",
    "        merged_df2[col] < 0,\n",
    "        (merged_df2[col] == 0) & (merged_df2[col + '_weekly'] > 0),\n",
    "        (merged_df2[col] == 0) & (merged_df2[col + '_weekly'] == 0),\n",
    "        (merged_df2[col] > 0) & (merged_df2[col + '_weekly'] == 0),\n",
    "        (merged_df2[col] > 0) & (merged_df2[col + '_weekly'] > 0),\n",
    "        (merged_df2[col] >= 0) & (merged_df2[col + '_weekly'].isnull() | (merged_df2[col + '_weekly'] == 0))\n",
    "    ]\n",
    "\n",
    "    # Resultados basados en las condiciones\n",
    "    resultados = [\n",
    "        0,  # Semana < 0\n",
    "        0,  # Semana = 0 y weekly > 0\n",
    "        100,  # Semana = 0 y weekly = 0\n",
    "        100,  # Semana > 0 y weekly = 0\n",
    "        merged_df2[col] / merged_df2[col + '_weekly'] * 100,  # Semana > 0 y weekly > 0\n",
    "        100  # Semana >= 0 y no hay weekly o weekly = 0\n",
    "    ]\n",
    "\n",
    "    # Aplicar las condiciones y almacenar en el diccionario\n",
    "    columna_temporal = pd.Series(0, index=merged_df2.index).astype(float)  # Inicializar con 0\n",
    "    for cond, res in zip(condiciones, resultados):\n",
    "        columna_temporal[cond] = res.clip(upper=100) if not isinstance(res, int) else res\n",
    "    \n",
    "    nuevas_columnas[nueva_col] = columna_temporal\n",
    "\n",
    "# Concatenar todas las nuevas columnas a merged_df2\n",
    "merged_df2 = pd.concat([merged_df2, pd.DataFrame(nuevas_columnas)], axis=1)\n",
    "\n",
    "# Eliminar las columnas que terminan en \"_weekly\"\n",
    "columnas_weekly = [col for col in merged_df2.columns if col.endswith('_weekly')]\n",
    "merged_df2.drop(columns=columnas_weekly, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "desktop_dir = os.path.join(os.path.expanduser('~'), 'OneDrive - Inchcape', 'Escritorio')\n",
    "merged_df2.to_excel(os.path.join(desktop_dir, 'DispoFutura.xlsx'), index=False)\n",
    "df_transito.to_excel(os.path.join(desktop_dir, 'TR_Semanal.xlsx'), index=False)\n",
    "weekly_values_df.to_excel(os.path.join(desktop_dir, 'FC_Semanal.xlsx'), index=False)\n",
    "df_stock.to_excel(os.path.join(desktop_dir, 'Stock_Tiendas.xlsx'), index=False)\n",
    "df_stock4.to_excel(os.path.join(desktop_dir, 'Stock_CD.xlsx'), index=False)\n",
    "FilteredUE711.to_excel(os.path.join(desktop_dir, 'Stock_711.xlsx'), index=False)\n",
    "print(\"finished\")\n",
    "\n",
    "# with pd.ExcelWriter(r'C:\\Users\\Etorres\\Desktop\\merged.xlsx', mode='a', engine='openpyxl') as writer:\n",
    "#     tabla_dinamica.to_excel(writer, sheet_name='Nueva_Hoja')\n",
    "\n",
    "# merged_df2.to_excel(r'C:\\Users\\Etorres\\Desktop\\merged7.xlsx', index=False)\n",
    "\n",
    "#df_transito.to_excel(r'C:\\Users\\Etorres\\Desktop\\tRANSITo6.xlsx', index=False)\n",
    "#weekly_values_df.to_excel(r'C:\\Users\\Etorres\\Desktop\\FC6.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Material', 'Descripción', 'Sector', 'Agrupación ClaCom', 'SubAgrupación ClaCom', 'Categoría ClaCom', 'SubCategoría ClaCom', 'Bobinas/Implementos/Otros', 'COD. PROVEEDOR', 'PROVEEDOR', 'LEAD TIME', 'Leadtime Semanal', 'ORIGEN', 'Comentario Sesión', 'Comentario Finales', 'VProm 3M', 'Codigos Inquebrables', 'Segmentacion', 'vig may', 'vig gt', 'vig retail', 'Vigencia Total', 'OBS Retail', 'OBS DERCO', 'Obs Total', 'Faltante AP', 'Faltante AGP', 'Faltantes', 'Sobrante AP', 'Sobrante AGP', 'Sobrantes', 'AP', 'AGP', 'Total Planograma', 'Stock Tiendas', 'Stock CD', 'Control', 'Stock 711', 'Sem 10 (24)', 'Sem 11 (24)', 'Sem 12 (24)', 'Sem 13 (24)', 'Sem 14 (24)', 'Sem 15 (24)', 'Sem 16 (24)', 'Sem 17 (24)', 'Sem 18 (24)', 'Sem 19 (24)', 'Sem 20 (24)', 'Sem 21 (24)', 'Sem 22 (24)', 'Sem 23 (24)', 'Sem 24 (24)', 'Sem 25 (24)', 'Sem 26 (24)', 'Sem 27 (24)', 'Sem 28 (24)', 'Sem 29 (24)', 'Sem 30 (24)', 'Sem 31 (24)', 'Sem 32 (24)', 'Sem 33 (24)', 'Sem 34 (24)', 'Sem 35 (24)', 'Sem 36 (24)', 'Sem 37 (24)', 'Sem 38 (24)', 'Sem 39 (24)', 'Sem 40 (24)', 'Sem 41 (24)', 'Sem 42 (24)', 'Sem 43 (24)', 'Sem 44 (24)', 'Sem 45 (24)', 'Sem 46 (24)', 'Sem 47 (24)', 'Sem 48 (24)', 'Sem 49 (24)', 'Sem 50 (24)', 'Sem 51 (24)', 'Sem 52 (24)', 'Sem 1 (25)', 'Sem 2 (25)', 'Sem 3 (25)', 'Sem 4 (25)', 'Sem 5 (25)', 'Sem 6 (25)', 'Sem 7 (25)', 'Sem 10 (24)D', 'Sem 11 (24)D', 'Sem 12 (24)D', 'Sem 13 (24)D', 'Sem 14 (24)D', 'Sem 15 (24)D', 'Sem 16 (24)D', 'Sem 17 (24)D', 'Sem 18 (24)D', 'Sem 19 (24)D', 'Sem 20 (24)D', 'Sem 21 (24)D', 'Sem 22 (24)D', 'Sem 23 (24)D', 'Sem 24 (24)D', 'Sem 25 (24)D', 'Sem 26 (24)D', 'Sem 27 (24)D', 'Sem 28 (24)D', 'Sem 29 (24)D', 'Sem 30 (24)D', 'Sem 31 (24)D', 'Sem 32 (24)D', 'Sem 33 (24)D', 'Sem 34 (24)D', 'Sem 35 (24)D', 'Sem 36 (24)D', 'Sem 37 (24)D', 'Sem 38 (24)D', 'Sem 39 (24)D', 'Sem 40 (24)D', 'Sem 41 (24)D', 'Sem 42 (24)D', 'Sem 43 (24)D', 'Sem 44 (24)D', 'Sem 45 (24)D', 'Sem 46 (24)D', 'Sem 47 (24)D', 'Sem 48 (24)D', 'Sem 49 (24)D', 'Sem 50 (24)D', 'Sem 51 (24)D', 'Sem 52 (24)D', 'Sem 1 (25)D', 'Sem 2 (25)D', 'Sem 3 (25)D', 'Sem 4 (25)D', 'Sem 5 (25)D', 'Sem 6 (25)D', 'Sem 7 (25)D']\n"
     ]
    }
   ],
   "source": [
    "# Convertir el índice de columnas a una lista y luego imprimir esa lista\n",
    "columns_list = merged_df2.columns.tolist()\n",
    "print(columns_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
